{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64bbc9c3",
   "metadata": {},
   "source": [
    "# Checking the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05d264c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is a ROCm-GPU detected?  True\n",
      "How many ROCm-GPUs are detected?  1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Is a ROCm-GPU detected? \", torch.cuda.is_available())\n",
    "print(\"How many ROCm-GPUs are detected? \", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "881577f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d06f0ed",
   "metadata": {},
   "source": [
    "# Login to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ceca1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36670537b19b4f1e9125e3d3a9b533aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8919cf1c06495e89ef10eafa072986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_falcon.py:   0%|          | 0.00/7.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\n",
      "- configuration_falcon.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90400c2a5b7d4aed8e7b173e64193a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_falcon.py:   0%|          | 0.00/56.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\n",
      "- modeling_falcon.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef18831550d647c3b5bfe14d7d9b2c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/17.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8e822de5e745898cd1de89464e64c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d91781286c4ce187472f5c53b63bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e80c2ad71e34475eb80a2af4c14d3caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3326aa03ca1407a9d829460f64f0fe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0dc51b34d534f2294109a6d86715f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ca02269d5447978cfa295a9f4d7104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5152f885a7db4cd1a7bf9503aac5d0eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.73M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e569cdb6969843dfa41814e3b45a6dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/281 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Base model and tokenizer names.\n",
    "base_model_name = \"tiiuae/falcon-7b\"\n",
    "\n",
    "# Load base model to GPU memory.\n",
    "device = \"cuda:0\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name, trust_remote_code=True,\n",
    "     torch_dtype=torch.float16\n",
    ").to(device)\n",
    "\n",
    "# Load tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0534909c",
   "metadata": {},
   "source": [
    "# Dataset for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c8e576f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c52b87359449059c872416066ef729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98cf133069734354a9fdbc146f39b580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001-9ad84bb9cf65a4(â€¦):   0%|          | 0.00/967k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d692335f50b74f7dab2014dfc5319b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "{'text': '<s>[INST] write me a 1000 words essay about deez nuts. [/INST] The Deez Nuts meme first gained popularity in 2015 on the social media platform Vine. The video featured a young man named Rodney Bullard, who recorded himself asking people if they had heard of a particular rapper. When they responded that they had not, he would respond with the phrase \"Deez Nuts\" and film their reactions. The video quickly went viral, and the phrase became a popular meme. \\n\\nSince then, Deez Nuts has been used in a variety of contexts to interrupt conversations, derail discussions, or simply add humor to a situation. It has been used in internet memes, in popular music, and even in politics. In the 2016 US presidential election, a 15-year-old boy named Brady Olson registered as an independent candidate under the name Deez Nuts. He gained some traction in the polls and even made appearances on national news programs.\\n\\nThe Deez Nuts meme has had a significant impact on popular culture. It has become a recognizable catchphrase that people use to add humor to everyday conversations. The meme has also been used to satirize politics and other serious issues. For example, in 2016, a group of activists in the UK used the phrase \"Deez Nuts for President\" as part of a campaign to encourage young people to vote in the EU referendum. </s><s>[INST] Rewrite the essay in a more casual way. Instead of sounding proffesional, sound like a college student who is forced to write the essay but refuses to do so in the propper way. Use casual words and slang when possible. [/INST] Yo, so you want me to write a 1000-word essay about Deez Nuts? Alright, fine. So, this whole thing started on Vine back in 2015. Some dude named Rodney Bullard made a video where he would ask people if they knew a rapper, and when they said no, he would hit them with the classic line: \"Deez Nuts!\" People loved it, and it became a viral meme.\\n\\nNowadays, Deez Nuts is used for all kinds of stuff. You can throw it out there to interrupt someone or just to be funny. It\\'s all over the internet, in music, and even in politics. In fact, during the 2016 US presidential election, a kid named Brady Olson registered as an independent candidate under the name Deez Nuts. He actually got some attention from the media and made appearances on TV and everything.\\n\\nThe impact of Deez Nuts on our culture is pretty huge. It\\'s become a thing that everyone knows and uses to add some humor to their everyday conversations. Plus, people have used it to make fun of politics and serious issues too. Like, in the UK, some groups of activists used the phrase \"Deez Nuts for President\" to encourage young people to vote in the EU referendum.\\n\\nThere you have it, a thousand words about Deez Nuts in a more casual tone. Can I go back to playing video games now? </s>'}\n"
     ]
    }
   ],
   "source": [
    "# Dataset for fine-tuning.\n",
    "training_dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "training_dataset = load_dataset(training_dataset_name, split = \"train\")\n",
    "\n",
    "# Check the data.\n",
    "print(training_dataset)\n",
    "\n",
    "# Dataset 11 is a QA sample in English.\n",
    "print(training_dataset[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b75cd0",
   "metadata": {},
   "source": [
    "# Fine-tuning params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1848e6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters for SFTTrainer.\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "         num_train_epochs = 1,\n",
    "         per_device_train_batch_size = 4,\n",
    "         gradient_accumulation_steps = 1,\n",
    "         optim = \"paged_adamw_32bit\",\n",
    "         save_steps = 50,\n",
    "         logging_steps = 50,\n",
    "         learning_rate = 4e-5,\n",
    "         weight_decay = 0.001,\n",
    "         fp16=False,\n",
    "         bf16=False,\n",
    "         max_grad_norm = 0.3,\n",
    "         max_steps = -1,\n",
    "         warmup_ratio = 0.03,\n",
    "         group_by_length = True,\n",
    "         lr_scheduler_type = \"constant\",\n",
    "         report_to = \"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5982c5a6",
   "metadata": {},
   "source": [
    "# Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b32d65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find the bitsandbytes ROCm binary at PosixPath('/home/edinj/repos/rocm-finetuning-tutorial/.venv/lib/python3.12/site-packages/bitsandbytes/libbitsandbytes_rocm64.so')\n",
      "Could not load bitsandbytes native library: /home/edinj/repos/rocm-finetuning-tutorial/.venv/lib/python3.12/site-packages/bitsandbytes/libbitsandbytes_cpu.so: cannot open shared object file: No such file or directory\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/edinj/repos/rocm-finetuning-tutorial/.venv/lib/python3.12/site-packages/bitsandbytes/cextension.py\", line 125, in <module>\n",
      "    lib = get_native_library()\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/edinj/repos/rocm-finetuning-tutorial/.venv/lib/python3.12/site-packages/bitsandbytes/cextension.py\", line 104, in get_native_library\n",
      "    dll = ct.cdll.LoadLibrary(str(binary_path))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/ctypes/__init__.py\", line 460, in LoadLibrary\n",
      "    return self._dlltype(name)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/ctypes/__init__.py\", line 379, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "OSError: /home/edinj/repos/rocm-finetuning-tutorial/.venv/lib/python3.12/site-packages/bitsandbytes/libbitsandbytes_cpu.so: cannot open shared object file: No such file or directory\n",
      "\n",
      "ROCm Setup failed despite ROCm being available. Please run the following command to get more information:\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      "Inspect the output of the command and see if you can locate ROCm libraries. You might need to add them\n",
      "to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n",
      "and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g++ (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0\n",
      "Copyright (C) 2023 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n",
      "trainable params: 18,874,368 || all params: 6,940,595,072 || trainable%: 0.2719\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "        lora_alpha = 16,\n",
    "        lora_dropout = 0.1,\n",
    "        r = 64,\n",
    "        bias = \"none\",\n",
    "        task_type = \"CAUSAL_LM\"\n",
    ")\n",
    "# View the number of trainable parameters.\n",
    "from peft import get_peft_model\n",
    "peft_model = get_peft_model(base_model, peft_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05d63a87",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SFTTrainer.__init__() got an unexpected keyword argument 'dataset_text_field'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Initialize an SFT trainer.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m sft_trainer = \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset_text_field\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_arguments\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Run the trainer.\u001b[39;00m\n\u001b[32m     12\u001b[39m sft_trainer.train()\n",
      "\u001b[31mTypeError\u001b[39m: SFTTrainer.__init__() got an unexpected keyword argument 'dataset_text_field'"
     ]
    }
   ],
   "source": [
    "# Initialize an SFT trainer.\n",
    "sft_trainer = SFTTrainer(\n",
    "        model = base_model,\n",
    "        train_dataset = training_dataset,\n",
    "        peft_config = peft_config,\n",
    "        dataset_text_field = \"text\",\n",
    "        tokenizer = tokenizer,\n",
    "        args = training_arguments\n",
    ")\n",
    "\n",
    "# Run the trainer.\n",
    "sft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460164b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT adapter name.\n",
    "adapter_name = \"llama-2-7b-enhanced-adapter\"\n",
    "\n",
    "# Save PEFT adapter.\n",
    "sft_trainer.model.save_pretrained(adapter_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
